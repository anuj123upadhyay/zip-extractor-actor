
import asyncio
import json
import logging
import os
import shutil
import zipfile
from pathlib import Path
from typing import Optional, Dict, List
from urllib.parse import urlparse
from datetime import datetime

import aiohttp
from apify import Actor

# Configure logging with detailed format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ZipDownloadExtractor:
    """High-performance ZIP downloader and extractor with advanced features."""
    
    def __init__(self, actor: Actor):
        self.actor = actor
        self.stats = {
            'total_downloaded': 0,
            'total_extracted': 0,
            'files_processed': 0,
            'errors': [],
            'start_time': None,
            'end_time': None,
            'skipped_files': 0,
            'corrupted_files': 0,
        }
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def download_file(
        self,
        url: str,
        output_path: str,
        timeout: int = 300,
        chunk_size: int = 8192,
        retries: int = 3
    ) -> bool:
        """Download file with retry logic and progress tracking."""
        for attempt in range(retries):
            try:
                logger.info(f"Starting download: {url} (Attempt {attempt + 1}/{retries})")
                
                timeout_obj = aiohttp.ClientTimeout(total=timeout)
                connector = aiohttp.TCPConnector(ssl=True, limit=10)  # SECURITY FIX: SSL enabled
                async with aiohttp.ClientSession(timeout=timeout_obj, connector=connector) as session:
                    async with session.get(url, allow_redirects=True) as response:
                        if response.status != 200:
                            error_msg = f"HTTP {response.status} for {url}"
                            logger.error(error_msg)
                            if attempt == retries - 1:
                                self.stats['errors'].append(error_msg)
                                return False
                            await asyncio.sleep(2 ** attempt)  # Exponential backoff
                            continue
                        
                        content_length = response.content_length or 0
                        downloaded = 0
                        
                        # Ensure directory exists
                        os.makedirs(os.path.dirname(output_path), exist_ok=True)
                        
                        with open(output_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(chunk_size):
                                if chunk:
                                    f.write(chunk)
                                    downloaded += len(chunk)
                                    
                                    # Push real-time progress
                                    if content_length:
                                        progress = min(100, int((downloaded / content_length) * 100))
                                        if progress % 10 == 0:  # Log every 10%
                                            await self.actor.push_data({
                                                'type': 'progress',
                                                'status': 'downloading',
                                                'url': url,
                                                'progress_percent': progress,
                                                'bytes_downloaded': downloaded,
                                                'total_bytes': content_length,
                                                'timestamp': datetime.now().isoformat(),
                                            })
                        
                        file_size = os.path.getsize(output_path)
                        self.stats['total_downloaded'] += file_size
                        logger.info(f"‚úì Downloaded {file_size:,} bytes from {url}")
                        return True
            
            except asyncio.TimeoutError:
                error_msg = f"Timeout downloading {url} (Attempt {attempt + 1}/{retries})"
                logger.warning(error_msg)
                if attempt == retries - 1:
                    self.stats['errors'].append(error_msg)
                    return False
                await asyncio.sleep(2 ** attempt)
            
            except Exception as e:
                error_msg = f"Error downloading {url}: {str(e)}"
                logger.error(error_msg)
                if attempt == retries - 1:
                    self.stats['errors'].append(error_msg)
                    return False
                await asyncio.sleep(2 ** attempt)
        
        return False
    
    def extract_zip(
        self,
        zip_path: str,
        extract_path: str,
        handle_duplicates: str = 'rename',
        password: Optional[str] = None,
        max_extraction_size: int = 10 * 1024 * 1024 * 1024  # 10GB default limit
    ) -> bool:
        """Extract ZIP with advanced features and safety checks."""
        try:
            logger.info(f"Extracting {zip_path} to {extract_path}")
            os.makedirs(extract_path, exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # Validate ZIP integrity
                bad_file = zip_ref.testzip()
                if bad_file:
                    logger.warning(f"ZIP integrity check failed on: {bad_file}")
                
                file_list = zip_ref.namelist()
                total_files = len(file_list)
                total_size = sum(zip_ref.getinfo(name).file_size for name in file_list)
                
                # Check extraction size
                if total_size > max_extraction_size:
                    error_msg = f"Extraction size {total_size:,} bytes exceeds limit {max_extraction_size:,} bytes"
                    logger.error(error_msg)
                    self.stats['errors'].append(error_msg)
                    return False
                
                logger.info(f"ZIP contains {total_files} files ({total_size:,} bytes)")
                
                # Set password if provided
                if password:
                    zip_ref.setpassword(password.encode())
                
                # Extract with progress and error handling
                for idx, file_info in enumerate(zip_ref.infolist()):
                    try:
                        # Security: Prevent path traversal attacks
                        normalized_path = os.path.normpath(file_info.filename)
                        if normalized_path.startswith('..') or os.path.isabs(normalized_path):
                            logger.warning(f"Skipping suspicious path: {file_info.filename}")
                            self.stats['skipped_files'] += 1
                            continue
                        
                        target_path = os.path.join(extract_path, normalized_path)
                        
                        # Handle duplicates - FIXED LOGIC
                        if os.path.exists(target_path):
                            if handle_duplicates == 'skip':
                                logger.info(f"Skipping duplicate: {file_info.filename}")
                                self.stats['skipped_files'] += 1
                                continue
                            elif handle_duplicates == 'rename':
                                # Handle full path with subdirectories correctly
                                dir_path = os.path.dirname(target_path)
                                filename = os.path.basename(target_path)
                                base, ext = os.path.splitext(filename)
                                counter = 1
                                while os.path.exists(target_path):
                                    new_filename = f"{base}_{counter}{ext}"
                                    target_path = os.path.join(dir_path, new_filename)
                                    counter += 1
                                logger.info(f"Renamed duplicate to: {os.path.basename(target_path)}")
                            # else: overwrite (default behavior)
                        
                        # Extract file
                        if file_info.is_dir():
                            os.makedirs(target_path, exist_ok=True)
                        else:
                            os.makedirs(os.path.dirname(target_path), exist_ok=True)
                            try:
                                with zip_ref.open(file_info) as source, open(target_path, 'wb') as target:
                                    shutil.copyfileobj(source, target)
                            except RuntimeError as e:
                                if 'Bad password' in str(e):
                                    error_msg = f"Bad password for encrypted file: {file_info.filename}"
                                    logger.error(error_msg)
                                    self.stats['errors'].append(error_msg)
                                    self.stats['corrupted_files'] += 1
                                    continue
                                raise
                        
                        # Progress tracking
                        progress = int(((idx + 1) / total_files) * 100)
                        if (idx + 1) % max(1, total_files // 10) == 0:  # Log every 10%
                            logger.info(f"Extraction progress: {progress}% ({idx + 1}/{total_files} files)")
                        
                        self.stats['total_extracted'] += 1
                    
                    except Exception as e:
                        error_msg = f"Error extracting {file_info.filename}: {str(e)}"
                        logger.error(error_msg)
                        self.stats['errors'].append(error_msg)
                        self.stats['corrupted_files'] += 1
            
            logger.info(f"‚úì Successfully extracted {self.stats['total_extracted']} files")
            return True
        
        except zipfile.BadZipFile:
            error_msg = f"Invalid or corrupted zip file: {zip_path}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False
        except PermissionError:
            error_msg = f"Permission denied: {zip_path}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False
        except Exception as e:
            error_msg = f"Unexpected error extracting {zip_path}: {str(e)}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False
    
    async def process_zip(
        self,
        url: str,
        extract_to_memory: bool = False,
        keep_zip: bool = False,
        password: Optional[str] = None,
        handle_duplicates: str = 'rename',
        timeout: int = 300
    ) -> Dict:
        """Main processing function with comprehensive error handling."""
        self.stats['start_time'] = asyncio.get_event_loop().time()
        self.stats['files_processed'] += 1
        
        # Use Apify storage for better reliability - FIXED
        # Use actor's temporary directory instead of hardcoded path
        temp_dir = os.path.join(os.getcwd(), 'apify_storage', 'temp')
        os.makedirs(temp_dir, exist_ok=True)
        
        try:
            # Extract filename from URL
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path) or 'downloaded.zip'
            
            # Sanitize filename
            filename = "".join(c for c in filename if c.isalnum() or c in ('.', '_', '-'))
            
            zip_path = os.path.join(temp_dir, filename)
            extract_path = os.path.join(temp_dir, 'extracted')
            
            # Download
            if not await self.download_file(url, zip_path, timeout=timeout):
                return {
                    'success': False,
                    'url': url,
                    'error': 'Failed to download file',
                    'filename': filename,
                    'timestamp': datetime.now().isoformat(),
                }
            
            # Extract
            if not self.extract_zip(zip_path, extract_path, handle_duplicates, password):
                return {
                    'success': False,
                    'url': url,
                    'error': 'Failed to extract zip',
                    'filename': filename,
                    'bytes_downloaded': os.path.getsize(zip_path),
                    'timestamp': datetime.now().isoformat(),
                }
            
            # Prepare result with detailed file information
            extracted_files = []
            if os.path.exists(extract_path):
                for root, dirs, files in os.walk(extract_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, extract_path)
                        file_size = os.path.getsize(file_path)
                        extracted_files.append({
                            'path': rel_path,
                            'size': file_size,
                            'type': Path(file_path).suffix,
                            'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                        })
            
            # Cleanup
            try:
                if not keep_zip and os.path.exists(zip_path):
                    os.remove(zip_path)
                    logger.info(f"Deleted temporary ZIP: {zip_path}")
                
                if extract_to_memory and os.path.exists(extract_path):
                    shutil.rmtree(extract_path)
                    logger.info(f"Deleted extraction folder: {extract_path}")
            except Exception as e:
                logger.warning(f"Cleanup error: {str(e)}")
            
            self.stats['end_time'] = asyncio.get_event_loop().time()
            
            return {
                'success': True,
                'url': url,
                'filename': filename,
                'files_extracted': len(extracted_files),
                'extracted_files': extracted_files,
                'bytes_downloaded': self.stats['total_downloaded'],
                'processing_time_seconds': round(self.stats['end_time'] - self.stats['start_time'], 2),
                'skipped_files': self.stats['skipped_files'],
                'corrupted_files': self.stats['corrupted_files'],
                'timestamp': datetime.now().isoformat(),
            }
        
        except Exception as e:
            logger.error(f"Unexpected error: {str(e)}", exc_info=True)
            self.stats['errors'].append(str(e))
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
            }


async def main():
    """Main actor function with robust error handling."""
    async with Actor:
        try:
            # Read input
            actor_input = await Actor.get_input()
            
            if not actor_input:
                actor_input = {}
            
            logger.info(f"Received input: {json.dumps(actor_input, indent=2)}")
            
            # Validate input - Enhanced empty input handling with default fallback
            urls_raw = actor_input.get('urls', [])
            
            # Extract URLs from requestListSources format
            urls = []
            if isinstance(urls_raw, list):
                for item in urls_raw:
                    if isinstance(item, dict) and 'url' in item:
                        # Extract URL string from dict
                        url_str = item.get('url')
                        if url_str and isinstance(url_str, str):
                            urls.append(url_str.strip())
                    elif isinstance(item, str):
                        # Direct string URL
                        urls.append(item.strip())
            elif isinstance(urls_raw, str):
                # Single string URL
                urls = [urls_raw.strip()]
            
            # Filter out empty strings
            urls = [url for url in urls if url]
            
            # Handle empty input - Use default URL for daily testing instead of failing
            if not urls:
                logger.warning("‚ö†Ô∏è No URLs provided in input. Using default test URL for demonstration.")
                logger.info("Expected format: {'urls': [{'url': 'https://example.com/file.zip'}]}")
                
                # Default URL for daily testing and demonstrations
                default_url = 'https://github.com/apify/apify-sdk-python/archive/refs/heads/master.zip'
                urls = [default_url]
                
                logger.info(f"üì¶ Using default test URL: {default_url}")
                logger.info("This ensures the actor can be tested without manual input configuration.")
            
            logger.info(f"Processing {len(urls)} URL(s): {urls}")
            
            # Process options with defaults
            extract_to_memory = actor_input.get('extract_to_memory', False)
            keep_zip = actor_input.get('keep_zip', False)
            password = actor_input.get('password')
            handle_duplicates = actor_input.get('handle_duplicates', 'rename')
            timeout = actor_input.get('timeout', 300)
            
            # Validate handle_duplicates option
            if handle_duplicates not in ['rename', 'skip', 'overwrite']:
                handle_duplicates = 'rename'
                logger.warning(f"Invalid handle_duplicates value, using default: {handle_duplicates}")
            
            # Create processor
            processor = ZipDownloadExtractor(Actor)
            
            # Process each URL
            results = []
            start_time = datetime.now()
            
            for idx, url in enumerate(urls, 1):
                logger.info(f"Processing URL {idx}/{len(urls)}: {url}")
                
                result = await processor.process_zip(
                    url=url,
                    extract_to_memory=extract_to_memory,
                    keep_zip=keep_zip,
                    password=password,
                    handle_duplicates=handle_duplicates,
                    timeout=timeout,
                )
                results.append(result)
                await Actor.push_data(result)
            
            end_time = datetime.now()
            
            # Push comprehensive summary
            summary = {
                'type': 'summary',
                'total_urls_processed': processor.stats['files_processed'],
                'successful_extractions': sum(1 for r in results if r.get('success')),
                'failed_extractions': sum(1 for r in results if not r.get('success')),
                'total_bytes_downloaded': processor.stats['total_downloaded'],
                'total_files_extracted': processor.stats['total_extracted'],
                'total_skipped_files': processor.stats['skipped_files'],
                'total_corrupted_files': processor.stats['corrupted_files'],
                'total_errors': len(processor.stats['errors']),
                'errors': processor.stats['errors'][:10],  # Limit to 10 most recent errors
                'processing_duration_seconds': round((end_time - start_time).total_seconds(), 2),
                'results': results,
                'timestamp': datetime.now().isoformat(),
            }
            
            logger.info(f"Final summary: {json.dumps(summary, indent=2)}")
            await Actor.push_data(summary)
        
        except Exception as e:
            logger.error(f"Critical error in main: {str(e)}", exc_info=True)
            await Actor.push_data({
                'success': False,
                'error': f"Critical error: {str(e)}",
                'timestamp': datetime.now().isoformat(),
            })


if __name__ == '__main__':
    asyncio.run(main())